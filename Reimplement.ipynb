{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mhcho\\miniforge3\\envs\\stat598\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# load packages\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_biased_dataset(train_ratio=0.95,test_ratio=0.5,Ntrain=1000,Nvalid=10,Ntest=100,seed=None,class0=4,class1=9):\n",
    "    \n",
    "    # fix the random seed for the fair comparison with baseline\n",
    "    generator   =   np.random.seed(seed)\n",
    "    \n",
    "    # use imbalanced training data set / balanced test data set\n",
    "    (train_images,train_labels),(test_images,test_labels)   =   tf.keras.datasets.mnist.load_data()\n",
    "    train_images_class0     =   train_images[train_labels==class0]\n",
    "    test_images_class0      =   test_images[test_labels==class0]\n",
    "    train_images_class1     =   train_images[train_labels==class1]\n",
    "    test_images_class1      =   test_images[test_labels==class1]\n",
    "    \n",
    "    # dtype conversion\n",
    "    train_images_class0     =   train_images_class0.astype(np.float32)/255\n",
    "    test_images_class0      =   test_images_class0.astype(np.float32)/255\n",
    "    train_images_class1     =   train_images_class1.astype(np.float32)/255\n",
    "    test_images_class1      =   test_images_class1.astype(np.float32)/255\n",
    "    \n",
    "    # random selection\n",
    "    print('Randomly select %d train / %d validation / %d test data in class %d' % (int(np.floor(train_ratio*Ntrain)),int(np.floor(test_ratio*Nvalid)),int(np.floor(test_ratio*Ntest)),class0))\n",
    "    train_images_class0_sel     =   train_images_class0[np.random.randint(0,train_images_class0.shape[0],int(np.floor(train_ratio*Ntrain))),:,:]\n",
    "    valid_images_class0_sel     =   train_images_class0[np.random.randint(0,train_images_class0.shape[0],int(np.floor(test_ratio*Nvalid))),:,:]\n",
    "    test_images_class0_sel      =   test_images_class0[np.random.randint(0,test_images_class0.shape[0],int(np.floor(test_ratio*Ntest))),:,:]\n",
    "\n",
    "    print('Randomly select %d train / %d validation / %d test data in class %d' % (int(Ntrain-np.floor(train_ratio*Ntrain)),int(Nvalid-np.floor(test_ratio*Nvalid)),Ntest-int(np.floor(test_ratio*Ntest)),class1))\n",
    "    train_images_class1_sel     =   train_images_class1[np.random.randint(0,train_images_class1.shape[0],int(Ntrain-np.floor(train_ratio*Ntrain))),:,:]\n",
    "    valid_images_class1_sel     =   train_images_class1[np.random.randint(0,train_images_class1.shape[0],int(Nvalid-np.floor(test_ratio*Nvalid))),:,:]\n",
    "    test_images_class1_sel      =   test_images_class1[np.random.randint(0,test_images_class1.shape[0],int(Ntest-np.floor(test_ratio*Ntest))),:,:]\n",
    "    \n",
    "    # concatenate class 0 and class 1 data\n",
    "    x_train_concatenate         =   np.concatenate([train_images_class0_sel,train_images_class1_sel],axis=0)\n",
    "    x_valid_concatenate         =   np.concatenate([valid_images_class0_sel,valid_images_class1_sel],axis=0)\n",
    "    x_test_concatenate          =   np.concatenate([test_images_class0_sel,test_images_class1_sel],axis=0)\n",
    "\n",
    "    y_train_concatenate         =   np.concatenate([np.zeros([train_images_class0_sel.shape[0]]),np.ones([train_images_class1_sel.shape[0]])])\n",
    "    y_valid_concatenate         =   np.concatenate([np.zeros([valid_images_class0_sel.shape[0]]),np.ones([valid_images_class1_sel.shape[0]])])\n",
    "    y_test_concatenate          =   np.concatenate([np.zeros([test_images_class0_sel.shape[0]]),np.ones([test_images_class1_sel.shape[0]])])\n",
    "\n",
    "    # reshape for CONV2D\n",
    "    x_train_concatenate         =   x_train_concatenate.reshape(-1,x_train_concatenate.shape[1],x_train_concatenate.shape[2],1)\n",
    "    x_valid_concatenate         =   x_valid_concatenate.reshape(-1,x_valid_concatenate.shape[1],x_valid_concatenate.shape[2],1)\n",
    "    x_test_concatenate          =   x_test_concatenate.reshape(-1,x_test_concatenate.shape[1],x_test_concatenate.shape[2],1)\n",
    "\n",
    "    # shuffle for training\n",
    "    idx     =   np.arange(x_train_concatenate.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    x_train         =   x_train_concatenate[idx]\n",
    "    y_train         =   y_train_concatenate[idx]\n",
    "\n",
    "    idx     =   np.arange(x_valid_concatenate.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    x_valid         =   x_valid_concatenate[idx]\n",
    "    y_valid         =   y_valid_concatenate[idx]\n",
    "\n",
    "    idx     =   np.arange(x_test_concatenate.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    x_test          =   x_test_concatenate[idx]\n",
    "    y_test          =   y_test_concatenate[idx]\n",
    "\n",
    "    return x_train, y_train, x_valid, y_valid, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet_model(inputs,labels,theta_dict=None,w=None,reuse=None,dtype=tf.float32):\n",
    "    \n",
    "    # check whether the parameters of the nueral network is specified, if not specified initialize with empty dictionary\n",
    "    if theta_dict is None:\n",
    "        theta_dict  =   {}\n",
    "\n",
    "    def _get_var(name,shape,dtype,initializer):\n",
    "        key     =   tf.get_variable_scope().name + '/' + name\n",
    "        \n",
    "        # check whether the given variable is created\n",
    "        if key in theta_dict:\n",
    "            return theta_dict[key]\n",
    "        else:\n",
    "            # create a new variable\n",
    "            var             =   tf.get_variable(name,shape,dtype,initializer=initializer)\n",
    "            theta_dict[key] =   var\n",
    "            return var\n",
    "\n",
    "    with tf.variable_scope('Model',reuse=reuse):\n",
    "        inputs_     =   tf.cast(tf.reshape(inputs,[-1,28,28,1]),dtype)\n",
    "        labels      =   tf.cast(labels,dtype)\n",
    "\n",
    "        # create weight\n",
    "        theta_init  =   tf.truncated_normal_initializer(mean=0,stddev=0.1)\n",
    "        theta1      =   _get_var('theta1',[5,5,1,16],dtype,initializer=theta_init)\n",
    "        theta2      =   _get_var('theta2',[5,5,16,32],dtype,initializer=theta_init)\n",
    "        theta3      =   _get_var('theta3',[5,5,32,64],dtype,initializer=theta_init)\n",
    "        theta4      =   _get_var('theta4',[1024,100],dtype,initializer=theta_init)\n",
    "        theta5      =   _get_var('theta5',[100,1], dtype,initializer=theta_init)\n",
    "\n",
    "        # create offset\n",
    "        b_init      =   tf.constant_initializer(0.0)\n",
    "        b1          =   _get_var('b1',[16],dtype,initializer=b_init)\n",
    "        b2          =   _get_var('b2',[32],dtype,initializer=b_init)\n",
    "        b3          =   _get_var('b3',[64],dtype,initializer=b_init)\n",
    "        b4          =   _get_var('b4',[100],dtype,initializer=b_init)\n",
    "        b5          =   _get_var('b5',[1],dtype,initializer=b_init)\n",
    "        \n",
    "        # create layers\n",
    "        # convolution layer level 1\n",
    "        #l0      =   tf.identity(inputs_,name='l0')\n",
    "        z1  =   tf.add(tf.nn.conv2d(inputs_,theta1,[1,1,1,1],'SAME'),b1,name='z1')\n",
    "        l1  =   tf.nn.relu(tf.nn.max_pool(z1,[1,3,3,1],[1,2,2,1],'SAME'),name='l1')\n",
    "\n",
    "        # convolution layer level 2\n",
    "        z2  =   tf.add(tf.nn.conv2d(l1,theta2,[1,1,1,1],'SAME'),b2,name='i2')\n",
    "        l2  =   tf.nn.relu(tf.nn.max_pool(z2,[1,3,3,1],[1,2,2,1],'SAME'),name='l2')\n",
    "\n",
    "        # convolution layer level 3\n",
    "        z3  =   tf.add(tf.nn.conv2d(l2,theta3,[1,1,1,1],'SAME'),b3,name='z3')\n",
    "        l3  =   tf.nn.relu(tf.nn.max_pool(z3,[1,3,3,1],[1,2,2,1],'SAME'),name='l3')\n",
    "\n",
    "        # fully connected layer level 4\n",
    "        z4  =   tf.add(tf.matmul(tf.reshape(l3,[-1,1024]),theta4),b4,name='z4')\n",
    "        l4  =   tf.nn.relu(z4,name='l4')\n",
    "\n",
    "        # FC-5\n",
    "        z5  =   tf.add(tf.matmul(l4,theta5),b5,name='z5')\n",
    "\n",
    "        logits  =   tf.squeeze(z5)\n",
    "        out     =   tf.sigmoid(logits)\n",
    "        \n",
    "        # multiply sample weight\n",
    "        if w is None:\n",
    "            # average loss\n",
    "            loss    =   tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "        else:\n",
    "            # weighted loss\n",
    "            loss    =   tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels)*w)\n",
    "    return theta_dict, loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweight_autodiff(input_bias,label_bias,input_valid,label_valid,bsize_bias,bsize_valid,eps=0.0,gate_gradients=1):\n",
    "\n",
    "    w_bias      =   tf.zeros([bsize_bias],dtype=tf.float32)\n",
    "    w_valid     =   tf.ones([bsize_valid],dtype=tf.float32)/float(bsize_valid)\n",
    "    \n",
    "    [theta_dict,loss_bias,logits_bias] = LeNet_model(input_bias,label_bias,w=w_bias,reuse=True)\n",
    "    var_names   =   theta_dict.keys()\n",
    "    var_list    =   [theta_dict[idx] for idx in var_names]\n",
    "    grads       =   tf.gradients(loss_bias,var_list,gate_gradients=gate_gradients)\n",
    "\n",
    "    var_list_new    =   [vv-gg for gg,vv in zip(grads,var_list)]\n",
    "    theta_dict_new  =   dict(zip(var_names,var_list_new))\n",
    "    [_,loss_valid,logits_valid] = LeNet_model(input_valid,label_valid,theta_dict=theta_dict_new,w=w_valid,reuse=True)\n",
    "    grads_w         =   tf.gradients(loss_valid,[w_bias],gate_gradients=gate_gradients)[0]\n",
    "    ex_weight       =   -grads_w\n",
    "    ex_weight_plus  =   tf.maximum(ex_weight,eps)\n",
    "    ex_weight_sum   =   tf.reduce_sum(ex_weight_plus)\n",
    "    ex_weight_sum   +=  tf.to_float(tf.equal(ex_weight_sum, 0.0))\n",
    "    ex_weight_norm  =   ex_weight_plus/ex_weight_sum\n",
    "    \n",
    "    return ex_weight_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly select 950 train / 5 validation / 50 test data in class 4\n",
      "Randomly select 50 train / 5 validation / 50 test data in class 9\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable Model/theta1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"c:\\Users\\mhcho\\miniforge3\\envs\\stat598\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3800, in _create_op_internal\n    ret = Operation(\n  File \"c:\\Users\\mhcho\\miniforge3\\envs\\stat598\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 797, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"c:\\Users\\mhcho\\miniforge3\\envs\\stat598\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 1750, in variable_v2\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"c:\\Users\\mhcho\\miniforge3\\envs\\stat598\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 70, in variable_op_v2\n    return gen_state_ops.variable_v2(\n  File \"c:\\Users\\mhcho\\miniforge3\\envs\\stat598\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1801, in _init_from_args\n    self._variable = state_ops.variable_op_v2(\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# build training model\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 30\u001b[0m     _,loss_c,logits_c     \u001b[38;5;241m=\u001b[39m   \u001b[43mLeNet_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw_\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     train_op                \u001b[38;5;241m=\u001b[39m   tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mMomentumOptimizer(lr,\u001b[38;5;241m0.9\u001b[39m)\u001b[38;5;241m.\u001b[39mminimize(loss_c)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# build evaluation model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[62], line 25\u001b[0m, in \u001b[0;36mLeNet_model\u001b[1;34m(inputs, labels, theta_dict, w, reuse, dtype)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# create weight\u001b[39;00m\n\u001b[0;32m     24\u001b[0m theta_init  \u001b[38;5;241m=\u001b[39m   tf\u001b[38;5;241m.\u001b[39mtruncated_normal_initializer(mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,stddev\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m theta1      \u001b[38;5;241m=\u001b[39m   \u001b[43m_get_var\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtheta1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtheta_init\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m theta2      \u001b[38;5;241m=\u001b[39m   _get_var(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtheta2\u001b[39m\u001b[38;5;124m'\u001b[39m,[\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m32\u001b[39m],dtype,initializer\u001b[38;5;241m=\u001b[39mtheta_init)\n\u001b[0;32m     27\u001b[0m theta3      \u001b[38;5;241m=\u001b[39m   _get_var(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtheta3\u001b[39m\u001b[38;5;124m'\u001b[39m,[\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m64\u001b[39m],dtype,initializer\u001b[38;5;241m=\u001b[39mtheta_init)\n",
      "Cell \u001b[1;32mIn[62], line 15\u001b[0m, in \u001b[0;36mLeNet_model.<locals>._get_var\u001b[1;34m(name, shape, dtype, initializer)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m theta_dict[key]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# create a new variable\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     var             \u001b[38;5;241m=\u001b[39m   \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     theta_dict[key] \u001b[38;5;241m=\u001b[39m   var\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m var\n",
      "File \u001b[1;32mc:\\Users\\mhcho\\miniforge3\\envs\\stat598\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:1616\u001b[0m, in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1600\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_variable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   1601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_variable\u001b[39m(name,\n\u001b[0;32m   1602\u001b[0m                  shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1614\u001b[0m                  synchronization\u001b[38;5;241m=\u001b[39mVariableSynchronization\u001b[38;5;241m.\u001b[39mAUTO,\n\u001b[0;32m   1615\u001b[0m                  aggregation\u001b[38;5;241m=\u001b[39mVariableAggregation\u001b[38;5;241m.\u001b[39mNONE):\n\u001b[1;32m-> 1616\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_variable_scope\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_variable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_get_default_variable_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1619\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1620\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1621\u001b[0m \u001b[43m      \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1622\u001b[0m \u001b[43m      \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1623\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1624\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcollections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcaching_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaching_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpartitioner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalidate_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_resource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcustom_getter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_getter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1630\u001b[0m \u001b[43m      \u001b[49m\u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1631\u001b[0m \u001b[43m      \u001b[49m\u001b[43msynchronization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1632\u001b[0m \u001b[43m      \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mhcho\\miniforge3\\envs\\stat598\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:1326\u001b[0m, in \u001b[0;36mVariableScope.get_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1325\u001b[0m   dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype\n\u001b[1;32m-> 1326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvar_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_variable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfull_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreuse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcaching_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaching_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartitioner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1338\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_resource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_getter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_getter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1341\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynchronization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1342\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mhcho\\miniforge3\\envs\\stat598\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:582\u001b[0m, in \u001b[0;36m_VariableStore.get_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    580\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m custom_getter(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcustom_getter_kwargs)\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 582\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_true_getter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m      \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m      \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m      \u001b[49m\u001b[43mreuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreuse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcollections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcaching_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaching_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpartitioner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalidate_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_resource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m      \u001b[49m\u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m      \u001b[49m\u001b[43msynchronization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m      \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mhcho\\miniforge3\\envs\\stat598\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:535\u001b[0m, in \u001b[0;36m_VariableStore.get_variable.<locals>._true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/part_0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vars:\n\u001b[0;32m    530\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    531\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo partitioner was provided, but a partitioned version of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    532\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable was found: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/part_0. Perhaps a variable of the same \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname was already created with partitioning?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_single_variable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreuse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcaching_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaching_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_resource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynchronization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mhcho\\miniforge3\\envs\\stat598\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:898\u001b[0m, in \u001b[0;36m_VariableStore._get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    893\u001b[0m   \u001b[38;5;66;03m# Throw away internal tf entries and only take a few lines. In some\u001b[39;00m\n\u001b[0;32m    894\u001b[0m   \u001b[38;5;66;03m# cases the traceback can be longer (e.g. if someone uses factory\u001b[39;00m\n\u001b[0;32m    895\u001b[0m   \u001b[38;5;66;03m# functions to create variables) so we take more than needed in the\u001b[39;00m\n\u001b[0;32m    896\u001b[0m   \u001b[38;5;66;03m# default case.\u001b[39;00m\n\u001b[0;32m    897\u001b[0m   tb \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tb \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow/python\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x[\u001b[38;5;241m0\u001b[39m]][:\u001b[38;5;241m5\u001b[39m]\n\u001b[1;32m--> 898\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m Originally defined at:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    899\u001b[0m                    (err_msg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(traceback\u001b[38;5;241m.\u001b[39mformat_list(tb))))\n\u001b[0;32m    900\u001b[0m found_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vars[name]\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shape\u001b[38;5;241m.\u001b[39mis_compatible_with(found_var\u001b[38;5;241m.\u001b[39mget_shape()):\n",
      "\u001b[1;31mValueError\u001b[0m: Variable Model/theta1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"c:\\Users\\mhcho\\miniforge3\\envs\\stat598\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3800, in _create_op_internal\n    ret = Operation(\n  File \"c:\\Users\\mhcho\\miniforge3\\envs\\stat598\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 797, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"c:\\Users\\mhcho\\miniforge3\\envs\\stat598\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 1750, in variable_v2\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"c:\\Users\\mhcho\\miniforge3\\envs\\stat598\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 70, in variable_op_v2\n    return gen_state_ops.variable_v2(\n  File \"c:\\Users\\mhcho\\miniforge3\\envs\\stat598\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1801, in _init_from_args\n    self._variable = state_ops.variable_op_v2(\n"
     ]
    }
   ],
   "source": [
    "# create imbalanced datatset\n",
    "# set the size of training/validation/test sets\n",
    "Ntrain  =   1000\n",
    "Nvalid  =   10\n",
    "Ntest   =   100\n",
    "\n",
    "# adjust the ratio of trainig/validation/test samples\n",
    "train_ratio =   0.95\n",
    "test_ratio  =   0.50\n",
    "\n",
    "# get the trainig/validation/test samples\n",
    "[x_train,y_train,x_valid,y_valid,x_test,y_test] =   get_random_biased_dataset(train_ratio,test_ratio,Ntrain,Nvalid,Ntest,seed=None,class0=4,class1=9)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # set learning rate, batch size,\n",
    "    lr      =   1e-3\n",
    "    bsize   =   100\n",
    "    \n",
    "    # place holder for DNN model input/output\n",
    "    x_      =   tf.placeholder(tf.float32,[None,x_train.shape[1]*x_train.shape[2]],name='x')\n",
    "    y_      =   tf.placeholder(tf.float32,[None],name='y')\n",
    "    x_val_  =   tf.placeholder(tf.float32,[None,x_valid.shape[1]*x_valid.shape[2]],name='x_val')\n",
    "    y_val_  =   tf.placeholder(tf.float32,[None],name='y_val')\n",
    "    w_      =   tf.placeholder(tf.float32,[None],name='w')\n",
    "    lr_     =   tf.placeholder(tf.float32,[],name='lr')\n",
    "\n",
    "    # build training model\n",
    "    with tf.name_scope('Train'):\n",
    "        [_,loss_c,logits_c]     =   LeNet_model(x_,y_,theta_dict=None,w=w_,reuse=None,dtype=tf.float32)\n",
    "        train_op                =   tf.train.MomentumOptimizer(lr,0.9).minimize(loss_c)\n",
    "\n",
    "    # build evaluation model\n",
    "    with tf.name_scope('Val'):\n",
    "        [_,loss_eval,logits_eval]   =   LeNet_model(x_,y_,theta_dict=None,w=w_,reuse=True,dtype=tf.float32)\n",
    "        prediction_                 =   tf.cast(tf.sigmoid(logits_eval)>0.5,tf.float32)\n",
    "        acc_                        =   tf.reduce_mean(tf.cast(tf.equal(prediction_,y_),tf.float32))\n",
    "\n",
    "    # build reweighting model\n",
    "    w_values_   =   reweight_autodiff(x_, y_,x_val_,y_val_,bsize,min(bsize,Nvalid),eps=0.0,gate_gradients=1)\n",
    "    \n",
    "\n",
    "\n",
    "#         num_steps = config.num_steps\n",
    "\n",
    "#         acc_sum = 0.0\n",
    "#         acc_test_sum = 0.0\n",
    "#         loss_sum = 0.0\n",
    "#         count = 0\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "#         for step in six.moves.xrange(num_steps):\n",
    "#             x, y = train_set.next_batch(bsize)\n",
    "#             x_val, y_val = val_set.next_batch(min(bsize, nval))\n",
    "\n",
    "#             # Use 50% learning rate for the second half of training.\n",
    "#             if step > num_steps // 2:\n",
    "#                 lr = config.lr / 2.0\n",
    "#             else:\n",
    "#                 lr = config.lr\n",
    "\n",
    "#             ex_weights = sess.run(\n",
    "#                 ex_weights_, feed_dict={x_: x,\n",
    "#                                         y_: y,\n",
    "#                                         x_val_: x_val,\n",
    "#                                         y_val_: y_val})\n",
    "#             loss, acc, _ = sess.run(\n",
    "#                 [loss_c, acc_, train_op],\n",
    "#                 feed_dict={\n",
    "#                     x_: x,\n",
    "#                     y_: y,\n",
    "#                     x_val_: x_val,\n",
    "#                     y_val_: y_val,\n",
    "#                     ex_wts_: ex_weights,\n",
    "#                     lr_: lr\n",
    "#                 })\n",
    "#             if (step + 1) % 100 == 0:\n",
    "#                 train_acc, test_acc = evaluate(sess, x_, y_, acc_, train_set, test_set)\n",
    "#                 if verbose:\n",
    "#                     print('Step', step + 1, 'Loss', loss, 'Train acc', train_acc, 'Test acc',\n",
    "#                           test_acc)\n",
    "#                 if FLAGS.tensorboard:\n",
    "#                     exp_logger.log(step + 1, 'train acc', train_acc)\n",
    "#                     exp_logger.log(step + 1, 'test acc', test_acc)\n",
    "#                     exp_logger.flush()\n",
    "#                 acc_sum = 0.0\n",
    "#                 loss_sum = 0.0\n",
    "#                 acc_test_sum = 0.0\n",
    "#                 count = 0\n",
    "\n",
    "#         # Final evaluation.\n",
    "#         train_acc, test_acc = evaluate(sess, x_, y_, acc_, train_set, test_set)\n",
    "#         if verbose:\n",
    "#             print('Final', 'Train acc', train_acc, 'Test acc', test_acc)\n",
    "#     return train_acc, test_acc\n",
    "\n",
    "# \n",
    "np.random.seed(5)\n",
    "print(np.random.rand(2,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat598",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
